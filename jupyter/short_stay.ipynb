{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import math\n",
    "import lightgbm as lgb\n",
    "from collections import Counter  \n",
    "import time\n",
    "from scipy.stats import kurtosis,iqr\n",
    "import seaborn as sns\n",
    "from scipy import ptp\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from sklearn.externals import joblib\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,log_loss,f1_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.externals import joblib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将short_stay和area_passenger_index拆分成多个小文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 北京市网格人流量指数,200*200网格\n",
    "# cols = ['date', 'hour', 'Grid_x_1', 'Grid_y_1', 'index_1']\n",
    "# short_stay = pd.read_csv('/home/poac/AnomalyDetectionDataset/Block_predict/shortstay_20200201_20200215.csv', names=cols, encoding='gb2312', sep='\\t')\n",
    "# tem = pd.read_csv('/home/poac/AnomalyDetectionDataset/Block_predict/shortstay_20200117_20200131.csv', names=cols, encoding='gb2312', sep='\\t')\n",
    "# short_stay = pd.concat([short_stay, tem], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_stay['date'] = short_stay['date'].apply(lambda x: ''.join(str(x)[0:4]+'-'+str(x)[4:6]+'-'+str(x)[6:8]))\n",
    "# short_stay['date'] = pd.to_datetime(short_stay['date'])\n",
    "# short_stay.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 按月，日将short_stay拆分成多个小文件\n",
    "# ### 定义起始日期为2020-01-17\n",
    "# start_date = short_stay['date'].iloc[0]-timedelta(days=15)\n",
    "# for i in tqdm(range(1, 31)):\n",
    "#     ### 取出一天的数据\n",
    "#     tem = short_stay[short_stay['date']==start_date]\n",
    "#     ### 索引重置\n",
    "#     tem = tem.reset_index(drop=True)\n",
    "#     ### 保存\n",
    "#     joblib.dump(tem, '/home/poac/AnomalyDetectionDataset/Block_predict/processed/short_stay_split/'+str(start_date)+'.jl.z')\n",
    "# #     tem.to_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/short_stay_split/short_stay_'+str(start_date)+'.csv', index=None)\n",
    "#     ### 日期加一\n",
    "#     start_date = start_date + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del short_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 读取区域人流量指数，含标签\n",
    "# ### 并按照日期将其拆分为多个小文件，分别保存\n",
    "# area_passenger_index = pd.read_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/area_passenger_index.csv')\n",
    "# area_passenger_index['date'] = pd.to_datetime(area_passenger_index['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 保存\n",
    "# start_date = area_passenger_index['date'].iloc[0]\n",
    "# for i in tqdm(range(1, 31)):\n",
    "#     ### 取出一天的数据\n",
    "#     tem = area_passenger_index[area_passenger_index['date']==start_date]\n",
    "#     ### 索引重置\n",
    "#     tem = tem.reset_index(drop=True)\n",
    "#     ### 保存\n",
    "#     tem.to_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/area_passenger_split/'+str(start_date)+'.csv')\n",
    "#     ### 日期加一\n",
    "#     start_date = start_date + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del area_passenger_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将input_for_sub拆成多个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_for_sub = pd.read_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/input_for_sub.csv')\n",
    "# input_for_sub['date'] = pd.to_datetime(input_for_sub['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 保存\n",
    "# start_date = input_for_sub['date'].iloc[0]\n",
    "# for i in tqdm(range(1, 10)):\n",
    "#     ### 取出一天的数据\n",
    "#     tem = input_for_sub[input_for_sub['date']==start_date]\n",
    "#     ### 索引重置\n",
    "#     tem = tem.reset_index(drop=True)\n",
    "#     ### 保存\n",
    "#     tem.to_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/input_for_sub_split/'+str(start_date)+'.csv')\n",
    "#     ### 日期加一\n",
    "#     start_date = start_date + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并area_passenger_index和short_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读取area_passenger_index所有文件\n",
    "files = os.listdir('/home/poac/AnomalyDetectionDataset/Block_predict/processed/area_passenger_split/')\n",
    "area_files = []\n",
    "for file in files:\n",
    "    if file[-4:] == '.csv':\n",
    "        area_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 读取short_stay所有文件\n",
    "files = os.listdir('/home/poac/AnomalyDetectionDataset/Block_predict/processed/short_stay_split/')\n",
    "short_files = []\n",
    "for file in files:\n",
    "    if file[-5:] == '.jl.z':\n",
    "        short_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 找对area_passenger_index中重点区域中心最近的坐标，并将其index添加\n",
    "### 先将area_passenger_index和short_stay merge\n",
    "area_passenger_index_processed = pd.DataFrame()\n",
    "for file in tqdm(area_files):\n",
    "    \n",
    "     ### 读取short_stay\n",
    "    short_stay = joblib.load('/home/poac/AnomalyDetectionDataset/Block_predict/processed/short_stay_split/'+file[0:-3]+'jl.z')\n",
    "    short_stay['date'] = short_stay['date'].astype(str)\n",
    "    ### 处理日期格式\n",
    "    short_stay['month'] = short_stay['date'].apply(lambda x: int(str(x)[5:7]))\n",
    "    short_stay['day'] = short_stay['date'].apply(lambda x: int(str(x)[8:]))\n",
    "    short_stay  = short_stay.drop(['date'], axis=1)\n",
    "    \n",
    "    ### 读取area_passenger_index\n",
    "    area_passenger_index = pd.read_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/area_passenger_split/'+file, chunksize=100)\n",
    "    for chunk in area_passenger_index:\n",
    "        chunk['date'] = chunk['date'].astype(str)\n",
    "        ### 处理日期格式\n",
    "        chunk['month'] = chunk['date'].apply(lambda x: int(str(x)[5:7]))\n",
    "        chunk['day'] = chunk['date'].apply(lambda x: int(str(x)[8:]))\n",
    "        chunk  = chunk.drop(['date', 'area_name', 'area_type'], axis=1)\n",
    "   \n",
    "        ### merge\n",
    "        chunk = chunk.merge(short_stay, how='left', on=['month', 'day', 'hour'])\n",
    "        chunk['distance'] = (chunk['Grid_x']-chunk['Grid_x_1'])**2 + (chunk['Grid_y']-chunk['Grid_y_1'])**2\n",
    "        ### 按照month,day,hour,distance排序\n",
    "        chunk= chunk.sort_values(by=['month', 'day', 'hour', 'distance'], ascending=[True, True, True, True])\n",
    "        ### 按照month,day,hour多列去重复，取第一次出现的数据，实现最小距离的获取\n",
    "        chunk = chunk.drop_duplicates(subset=['month','day', 'hour'],keep='first')\n",
    "        ### 处理一个chunk之后合并\n",
    "        area_passenger_index_processed = pd.concat([area_passenger_index_processed, chunk], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 处理完之后再去一次重\n",
    "area_passenger_index_processed = area_passenger_index_processed.drop_duplicates(subset=['month','day', 'hour'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 将处理完的数据保存\n",
    "area_passenger_index_processed.to_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/area_passenger_index_processed.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将short_stay和input_for_sub合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 读取area_passenger_index所有文件\n",
    "# files = os.listdir('/home/poac/AnomalyDetectionDataset/Block_predict/processed/input_for_sub_split/')\n",
    "# input_files = []\n",
    "# for file in files:\n",
    "#     if file[-4:] == '.csv':\n",
    "#         input_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### input_for_sub，并将其index添加\n",
    "# ### 先将input_for_sub和short_stay merge\n",
    "# input_for_sub = pd.DataFrame()\n",
    "# for file in input_files:\n",
    "    \n",
    "#      ### 读取short_stay\n",
    "#     short_stay = joblib.load('/home/poac/AnomalyDetectionDataset/Block_predict/processed/short_stay_split/'+file[0:-3]+'jl.z')\n",
    "#     short_stay['date'] = short_stay['date'].astype(str)\n",
    "#     ### 处理日期格式\n",
    "#     short_stay['month'] = short_stay['date'].apply(lambda x: int(str(x)[5:7]))\n",
    "#     short_stay['day'] = short_stay['date'].apply(lambda x: int(str(x)[8:]))\n",
    "#     short_stay  = short_stay.drop(['date'], axis=1)\n",
    "    \n",
    "#     ### 读取input_for_sub\n",
    "#     input_ = pd.read_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/input_for_sub_split/'+file, chunksize=100)\n",
    "#     for chunk in input_:\n",
    "#         chunk['date'] = chunk['date'].astype(str)\n",
    "#         ### 处理日期格式\n",
    "#         chunk['month'] = chunk['date'].apply(lambda x: int(str(x)[5:7]))\n",
    "#         chunk['day'] = chunk['date'].apply(lambda x: int(str(x)[8:]))\n",
    "#         chunk  = chunk.drop(['date', 'area_name', 'area_type'], axis=1)\n",
    "   \n",
    "#         ### merge\n",
    "#         chunk = chunk.merge(short_stay, how='left', on=['month', 'day', 'hour'])\n",
    "#         chunk['distance'] = (chunk['Grid_x']-chunk['Grid_x_1'])**2 + (chunk['Grid_y']-chunk['Grid_y_1'])**2\n",
    "#         ### 按照month,day,hour,distance排序\n",
    "#         chunk= chunk.sort_values(by=['month', 'day', 'hour', 'distance'], ascending=[True, True, True, True])\n",
    "#         ### 按照month,day,hour多列去重复，取第一次出现的数据，实现最小距离的获取\n",
    "#         chunk = chunk.drop_duplicates(subset=['month','day', 'hour'],keep='first')\n",
    "#         ### 处理一个chunk之后合并\n",
    "#         input_for_sub = pd.concat([input_for_sub, chunk], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_for_sub = input_for_sub.drop_duplicates(subset=['month','day', 'hour'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 保存\n",
    "# input_for_sub.to_csv('/home/poac/AnomalyDetectionDataset/Block_predict/processed/input_for_sub_processed.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
